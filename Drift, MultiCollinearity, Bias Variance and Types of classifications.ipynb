{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias, Variance, Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Underfitting --> High Training error and high bias and high variance. Bias is error of the training data. Variance error of test data\n",
    "- Overfitting --> High Test error and low bias and high variance. \n",
    "- Correct fit --> low bias and low variance\n",
    "\n",
    "A decision tree with its complete depth is overfitting coz it has high variance and low bias. That's why trees we prune at some depth. In random forest multiple trees are used, so since we combine trees in parallel the high variance will turn to low variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiCollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When multiple independent features are highly correlated and they behave linearly, the scenario is called as multicollinearity. The other way this can be defined is that one independent features can be easily predicted using another independent feature. This is not favoured in a model because it increases the dimensionality and leads to high bias only. Multicollinear features can be identified using VIF (variable inflation factor). The following are few reasons why correlated features should not be used in a model:\n",
    "\n",
    "- Linear Regression\n",
    "\n",
    "a. The assumption of linear regression is that there should be no linearly related features.\n",
    "b. Reason being, the weights of the coefficients which the model will try to assign will be split up or weightage will be disturbed. \n",
    "\n",
    "- Random forest or tree models\n",
    "\n",
    "a. The feature importance will be affected.\n",
    "b. Coz the tree will be split up using both the correlated features and the weightage of those features will be split up, resulting in not getting the proper weight for the most impactful feature.\n",
    "\n",
    "The krish naik video on multicollinearity was not helpful this time. It showed only correlation of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Types of Classifications.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a detailed explanation of each type of classification and to know which models support each of the above types of classification and how to implement them, visit: https://scikit-learn.org/stable/modules/multiclass.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Simplest explanation of drift is as follows (from Sundar):\n",
    "\n",
    "30 years back,\n",
    "Person who had telephone, car and own house is considered as Rich people.\n",
    "\n",
    "Person having Motor cycle, telephone and with or without own house are considered as middle income group.\n",
    "\n",
    "and others without these parameters are considered as low income group.\n",
    "\n",
    "In present day, this model of classification is not right because we have many people who have car and own house and still they may not considered as Rich people.\n",
    "\n",
    "Here we see 2 kinds of drifts:\n",
    "\n",
    "1. Concept drift - change in the target definition (Rich, middle income group or poor), we can have several new categories like Very high income group, high income group, middle class group, low income group and very low income group.\n",
    "\n",
    "2. Data Drift - AKA, change in the input features to define the income group target. Consider just one input feature for example as having_car. 30 years back if you have car, you were classified as Rich person however now a days almost every other person is having car, also person having Tata Indica and person having Audi are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Address Concept Drift?\n",
    "\n",
    "There are many ways to address concept drift; let’s take a look at a few.\n",
    "\n",
    "1. Do Nothing (Static Model)\n",
    "\n",
    "The most common way is to not handle it at all and assume that the data does not change. This allows you to develop a single “best” model once and use it on all future data. This should be your starting point and baseline for comparison to other methods. If you believe your dataset may suffer concept drift, you can use a static model in two ways:\n",
    "\n",
    "a.) Concept Drift Detection. Monitor skill of the static model over time and if skill drops, perhaps concept drift is occurring and some intervention is required.\n",
    "\n",
    "b.) Baseline Performance. Use the skill of the static model as a baseline to compare to any intervention you make.\n",
    "\n",
    "2. Periodically Re-Fit\n",
    "\n",
    "A good first-level intervention is to periodically update your static model with more recent historical data. For example, perhaps you can update the model each month or each year with the data collected from the prior period. This may also involve back-testing the model in order to select a suitable amount of historical data to include when re-fitting the static model. In some cases, it may be appropriate to only include a small portion of the most recent historical data to best capture the new relationships between inputs and outputs (e.g. the use of a sliding window).\n",
    "\n",
    "3. Periodically Update\n",
    "\n",
    "Some machine learning models can be updated. This is an efficiency over the previous approach (periodically re-fit) where instead of discarding the static model completely, the existing state is used as the starting point for a fit process that updates the model fit using a sample of the most recent historical data. For example, this approach is suitable for most machine learning algorithms that use weights or coefficients such as regression algorithms and neural networks.\n",
    "\n",
    "4. Weight Data\n",
    "\n",
    "Some algorithms allow you to weigh the importance of input data.In this case, you can use a weighting that is inversely proportional to the age of the data such that more attention is paid to the most recent data (higher weight) and less attention is paid to the least recent data (smaller weight).\n",
    "\n",
    "5. Learn The Change\n",
    "\n",
    "An ensemble approach can be used where the static model is left untouched, but a new model learns to correct the predictions from the static model based on the relationships in more recent data. This may be thought of as a boosting type ensemble (in spirit only) where subsequent models correct the predictions from prior models. The key difference here is that subsequent models are fit on different and more recent data, as opposed to a weighted form of the same dataset, as in the case of AdaBoost and gradient boosting.\n",
    "\n",
    "6. Detect and Choose Model\n",
    "\n",
    "For some problem domains it may be possible to design systems to detect changes and choose a specific and different model to make predictions. This may be appropriate for domains that expect abrupt changes that may have occurred in the past and can be checked for in the future. It also assumes that it is possible to develop skillful models to handle each of the detectable changes to the data. For example, the abrupt change may be a specific observation or observations in a range, or the change in the distribution of one or more input variables.\n",
    "\n",
    "7. Data Preparation\n",
    "\n",
    "In some domains, such as time series problems, the data may be expected to change over time. In these types of problems, it is common to prepare the data in such a way as to remove the systematic changes to the data over time, such as trends and seasonality by differencing. This is so common that it is built into classical linear methods like the ARIMA model. Typically, we do not consider systematic change to the data as a problem of concept drift because it can be dealt with directly. Rather, these examples may be a useful way of thinking about your problem and may help you anticipate change and prepare data in a specific way using standardization, scaling, projections, and more to mitigate or at least reduce the effects of change to input variables in the future.\n",
    "\n",
    "Futher reading: https://machinelearningmastery.com/gentle-introduction-concept-drift-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
