{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WoE (Weight of Evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 2 ideas – the weight of evidence (WOE) and information value (IV) evolved from the same logistic regression technique.\n",
    "\n",
    "We bin/group values of variables (continuous and categorical) in line with the subsequent rules:\n",
    "\n",
    "- First, each bin should have at least 5% of the observations\n",
    "- Second, each bin should be non-zero for both non-events and events\n",
    "- Third, The WOE should be monotonic, i.e. either growing or decreasing with the groupings\n",
    "- Last, we should bin missing values separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. Since it evolved from credit scoring world, it is generally described as a measure of the separation of good and bad customers. \"Bad Customers\" refers to the customers who defaulted on a loan. and \"Good Customers\" refers to the customers who paid back loan.\n",
    "\n",
    "WOE Calculation = ln(distribution of goods/distribution of bads)\n",
    "\n",
    "Distribution of Goods - % of Good Customers in a particular group\n",
    "Distribution of Bads - % of Bad Customers in a particular group\n",
    "ln - Natural Log\n",
    "\n",
    "Positive WOE means Distribution of Goods > Distribution of Bads\n",
    "Negative WOE means Distribution of Goods < Distribution of Bads\n",
    "Hint : Log of a number > 1 means positive value. If less than 1, it means negative value.\n",
    "\n",
    "###### Usage of WOE\n",
    "\n",
    "Weight of Evidence (WOE) helps to transform a continuous independent variable into a set of groups or bins based on similarity of dependent variable distribution i.e. number of events and non-events.\n",
    "\n",
    "###### Benefits of WOE\n",
    "\n",
    "- It can treat outliers. Suppose you have a continuous variable such as annual salary and extreme values are more than 500 million dollars. These values would be grouped to a class of (let's say 250-500 million dollars). Later, instead of using the raw values, we would be using WOE scores of each classes.\n",
    "- It can handle missing values as missing values can be binned separately.\n",
    "- Since WOE Transformation handles categorical variable so there is no need for dummy variables.\n",
    "- WoE transformation helps you to build strict linear relationship with log odds. Otherwise it is not easy to accomplish linear relationship using other transformation methods such as log, square-root etc. In short, if you would not use WOE transformation, you may have to try out several transformation methods to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Value (IV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information value is one of the most useful technique to select important variables in a predictive model. It helps to rank variables on the basis of their importance. The IV is calculated using the following formula :\n",
    "\n",
    "IV = ∑ (% of non-events - % of events) * WOE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the IV statistic is:\n",
    "\n",
    "- Less than 0.02, then the predictor is not useful for modeling (separating the Goods from the Bads)\n",
    "- 0.02 to 0.1, then the predictor has only a weak relationship to the Goods/Bads odds ratio\n",
    "- 0.1 to 0.3, then the predictor has a medium strength relationship to the Goods/Bads odds ratio\n",
    "- 0.3 to 0.5, then the predictor has a strong relationship to the Goods/Bads odds ratio.\n",
    "- greather than 0.5, suspicious relationship (Check once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Important Points\n",
    "\n",
    "- Information value increases as bins / groups increases for an independent variable. Be careful when there are more than 20 bins as some bins may have a very few number of events and non-events.\n",
    "- Information value is not an optimal feature (variable) selection method when you are building a classification model other than binary logistic regression (for eg. random forest or SVM) as conditional log odds (which we predict in a logistic regression model) is highly related to the calculation of weight of evidence. In other words, it's designed mainly for binary logistic regression model. Also think this way - Random forest can detect non-linear relationship very well so selecting variables via Information Value and using them in random forest model might not produce the most accurate and robust predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://gnpalencia.org/optbinning/tutorials/tutorial_binary.html\n",
    "\n",
    "Refer google colab for practical implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
